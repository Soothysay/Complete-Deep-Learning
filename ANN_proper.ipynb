{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN proper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "12M1d8STA7V65-Mkr5aJz8iZoADPe6VDq",
      "authorship_tag": "ABX9TyPaagQb3Xu38kRGVMn2D8C4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soothysay/Complete-Deep-Learning/blob/master/ANN_proper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjEx_qG1jOX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import utils\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Activation\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import datasets\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import History\n",
        "\n",
        "from keras import losses\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRTM7J5ssd2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv('/content/drive/My Drive/ANN/Churn_Modelling.csv')\n",
        "\n",
        "\n",
        "# list of columns whose data type is object i.e. string\n",
        "listOfColumnNames = list((df.dtypes[df.dtypes == np.object]).index)\n",
        "\n",
        "#filteredColumns = df.dtypes[df.dtypes == np.object]\n",
        "#listOfColumnNames = list(filteredColumns.index)\n",
        "\n",
        "\n",
        "#Removing the Name and Surname Columns (You can add more redundant parameters)\n",
        "if 'Name' in listOfColumnNames:\n",
        "  df=df.drop(['Name'],axis=1)\n",
        "if 'Surname' in listOfColumnNames:\n",
        "  df=df.drop(['Surname'],axis=1)\n",
        "if 'Name' in listOfColumnNames:\n",
        "  listOfColumnNames.remove('Name')\n",
        "if 'Surname' in listOfColumnNames:\n",
        "  listOfColumnNames.remove('Surname')\n",
        "\n",
        "\n",
        "#Replacing the object type values with classified values (Performance can be improved)\n",
        "for i in range(len(listOfColumnNames)):\n",
        "  s=set(df[listOfColumnNames[i]])\n",
        "  s=list(s)\n",
        "  for j in range(len(s)):\n",
        "    df=df.replace(to_replace=s[j],value=(j+1))\n",
        "\n",
        "col=len(df.columns)\n",
        "#Done with data shaping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKTVuD42lA5e",
        "colab_type": "text"
      },
      "source": [
        "24/4/2020: 7:56pm: Finally solved the problem I was plagued with for the past 2 days. Will take a break."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewf4pwBLxdhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Division of X and Y\n",
        "x=df.iloc[:,0:(col-1)]\n",
        "y=df.iloc[:,(col-1)]\n",
        "\n",
        "\n",
        "#Classifying the number of outputs (Sigmoid will be output AF in outer node, or else softmax)\n",
        "fin=set(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrA3QUZhlavz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Inputs I plan to take\n",
        "layer=int(input(\"Enter number of hidden layers: \")) #Number of hidden layers (between 2 to 9)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Enter the number of nodes in Each layer: \") #Automate number of nodes in each layer\n",
        "a = [ int(\n",
        "    input(\"Enter the number of nodes in layer: \")) \n",
        "for i in range(layer)]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoAGEt2vi-Ib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fix random seed for reproducibility\n",
        "np.random.seed(12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K1oVdjjjKu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Will be needed later\n",
        "epochs=60\n",
        "learning_rate = 0.1\n",
        "decay_rate = learning_rate / epochs\n",
        "momentum = 0.8\n",
        "\n",
        "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll5JuPSr2Rsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform(x_test)\n",
        "#Input Dimensions for the input layer of the ANN\n",
        "inputdim=len(x_train[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEOmKlE27IRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Building the ANN Model\n",
        "classifier = Sequential()\n",
        "i=1\n",
        "for i in range(1,layer):\n",
        "  print(i)\n",
        "  if(i==1):\n",
        "    classifier.add(Dense(units= a[i], kernel_initializer= \"he_uniform\",activation=\"relu\",input_dim = inputdim))\n",
        "    print(\"1\",i)\n",
        "  if(i!=1):\n",
        "    classifier.add(Dense(units = a[i], kernel_initializer = \"he_uniform\",activation=\"relu\"))\n",
        "    print(\"2\",i)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#classifier.add(Dense(output_dim = 6, kernel_initializer= 'he_uniform',activation='relu',input_dim = inputdim))\n",
        "#classifier.add(Dense(output_dim = 6, kernel_initializer = 'he_uniform',activation='relu'))\n",
        "\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(output_dim = 1, init = 'glorot_uniform', activation = 'sigmoid'))\n",
        "\n",
        "# Compiling the ANN\n",
        "classifier.compile(optimizer = 'Adamax', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the ANN to the Training set\n",
        "model_history=classifier.fit(x_train, y_train,validation_split=0.33, batch_size = 10, nb_epoch = 100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrGOqyqH8D3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list all data in history\n",
        "print(model_history.history.keys())\n",
        "plt.plot(model_history.history['accuracy'])\n",
        "plt.plot(model_history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD96idYy8ifB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(model_history.history['loss'])\n",
        "plt.plot(model_history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMxvTpjL8q2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(x_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print('confusion Matrix=',cm)\n",
        "# Calculate the Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "score=accuracy_score(y_pred,y_test)\n",
        "print('Score=',score)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}